\documentclass[xcolor=dvipsnames]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{soul}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{cancel}
\usepackage[percent]{overpic}
\usepackage{listings}
\usepackage{color}
\lstset{ %
  language=Python,
  basicstyle=\ttfamily\tiny,
  emphstyle=\color{red},
  keywordstyle=\color{black}\bfseries,
  identifierstyle=\color{DarkOrchid}\ttfamily,
  commentstyle=\color{Brown}\rmfamily\itshape,
  stringstyle=\color{blue}\slshape,
  showstringspaces=false,
  frame=single,                   % adds a frame around the code
}

\setbeamertemplate{navigation symbols}{}
\useoutertheme{infolines}
\usecolortheme[named=violet]{structure}
\setbeamertemplate{items}[circle]

\DeclareGraphicsExtensions{.pdf,.png,.jpg}
\title[Data Handling]{CCD Acceptance Testing Data Handling}
\author{Brett Viren and Tom Throwe}
\institute[BNL]
{
  Physics Department

  \includegraphics[height=1.5cm]{bnl-logo}

  \includegraphics[height=1.5cm]{dyb_logo}
}

\date{\today}

\definecolor{rootpink}{RGB}{255,0,255}
\definecolor{macroyellow}{RGB}{255,215,0}

\hypersetup{
  pdfkeywords={},
  pdfsubject={},
  pdfcreator={pdflatex, beamer and emacs the digital blood, sweat and tears}}

\begin{document}

\maketitle

\begin{frame}[fragile]
  \frametitle{High Level Data Flow}
  \includegraphics[width=\textwidth]{dataflow}
  \begin{enumerate}
  \item Test station produces result files.
  \item Files are stored to location on disk.
  \item Validated and uploaded to database.
  \end{enumerate}

  Intermediate files allow loose coupling between varied test station
  platforms and database.
\end{frame}

\begin{frame}
  \frametitle{Job Harness - Version Control}
  \begin{columns}
    \begin{column}{0.5\paperwidth}
      \begin{itemize}
      \item Tests run in a common job harness (Linux and Windows).
      \item Operator enters external parameters defining a run.
      \item Station software from a specific commit checked out.
      \item Test jobs run producing results.
      \end{itemize}
    \end{column}
    \begin{column}{0.5\paperwidth}
      \includegraphics[width=\textwidth]{job-harness}
    \end{column}
  \end{columns}

  $\rightarrow$Currently at the conceptual level.
\end{frame}

\begin{frame}
  \frametitle{Station Results}

  A station produces result files in three categories:

  \begin{columns}
    \begin{column}{0.5\paperwidth}
      \begin{description}
      \item[Metadata FITS] Describes test software and enumerates results
        files and any auxiliary files.
      \item[Result FITS] Result summary files to be parsed and uploaded
        into the DB.
      \item[Auxiliary] Any additional files or arbitrary type to archive.
      \end{description}
    \end{column}
    \begin{column}{0.45\paperwidth}
      \includegraphics[width=\textwidth]{files}
    \end{column}
  \end{columns}
  All files are archived, first two types are parsed into LIMS.

\end{frame}

\begin{frame}
  \frametitle{Metadata FITS File} 

  Collects information about all result files.  Here a ``test'' means
  a run on a test station or an offline analysis.  4 HDUs:

  \begin{enumerate}
  \item Primary HDU, header only.
    \begin{description}
    \item[\texttt{TESTNAME}] a canonical name for the test.
    \item[\texttt{DATE\_OBS}] UTC time stamp for when the test was run.
    \item[\texttt{USERNAME}] Name for the test station operator or analyzer.
    \end{description}
  \item Test's software description consisting of list of main
    programs and their GIT SHA1 commit hash and tag.  Lists are stored
    in a FITS table.
  \item List of FITS files to be parsed by LIMS (see next).  Stored as
    a table of filenames and content SHA1 digest hashes.
  \item List of all auxiliary files to be linked into the LIMS
    database.  Lists stored as HDU 3.
  \end{enumerate}

\end{frame}


\begin{frame}
  \frametitle{Result FITS File}
  Collects all result data that will be parsed into LIMS.

  Well defined schema:
  \begin{enumerate}
  \item Common PrimaryHDU for all tests.
    \begin{description}
    \item[\texttt{TESTNAME}] a canonical name for the test.
    \item[\texttt{EXTNAME}] HDU schema name
    \item[\texttt{SCHEMAVER}] schema version
    \end{description}
  \item Test-specific secondary HDUs with some common cards:
    \begin{description}
    \item[\texttt{EXTNAME}] name of schema this HDU follows
    \item[\texttt{SCHEMAVER}] version of schema this HDU follows
    \end{description}
  \end{enumerate}

  Many secondary HDUs will be identical and shared between different
  result FITS schema.

  A table/data-only HDU still must provide a standard, minimal set of
  cards in the header.
\end{frame}

\begin{frame}
  \frametitle{Example - JimF's Gain/Noise/ColdSpot}
  This result fits into 5 HDUs:
  \begin{description}
  \item[\texttt{Primary}] Common primary HDU
  \item[\texttt{FileRefs}] Table HDU holding columns for file paths and
    one for file checksum (SHA1) to record input files.
  \item[\texttt{Gains}] Table HDU holding two columns of floats, one
    for each gain measurement type.
  \item[\texttt{Noises}] Table HDU holding two columns of floats, one
    for each noise measurement type.
  \item[\texttt{ColdSpot}] Table HDU holding four columns of ints,
    amp\#, pixel count, spot X and spot Y.
  \end{description}

\end{frame}


\section{Schema Representation In Python}

\begin{frame}
  \frametitle{Schema Definition in Python}

  Steps to defining and validating a custom FITS schema
  \begin{enumerate}
  \item Adopt \texttt{pyfits} classes to get valid FITS format by-construction.
  \item Enforce our specific schema by adding a \texttt{validate()} method:
    \begin{itemize}
    \item existing \texttt{pyfits.*HDU} classes (bolt-on) for common validation
    \item subclass \texttt{pyfits.*HDU} classes for test-specific validation
    \end{itemize}
  \item Implement a Python module for each test stations results that
    hard-codes its specific schema.
  \end{enumerate}

\end{frame}

\begin{frame}
  \frametitle{Current Status of Implementation}

  Some basic validations written, JimF's example has been the only driving
  case so far (see below).

  \vspace{2mm}

  Some items still to do on the short term:

  \begin{itemize}
  \item Implement versioning to allow schema evolution, if needed (?)
  \item Emit SQL for table creation.  Ties in to schema evolution.
  \end{itemize}

\end{frame}


\begin{frame}[fragile]
  \frametitle{The LSST CCD Acceptance Testing Results package: \texttt{lcatr}}
  \begin{description}
  \item[\texttt{lcatr.schema}] import all of \texttt{pyfits} and
    bolt-on the \texttt{validate()} methods.
    \begin{itemize}
    \item [$\rightarrow$] code should do ``\texttt{import lcatr.schema}'' before using any \texttt{pyfits} objects.
    \end{itemize}
  \item[\texttt{lcatr.gnc}] definitions for JimF's Gain/Noise/ColdSpots results
  \item[\texttt{lcatr.<test>}] each station will have its own in a similar manner
  \end{description}

  For now, code is maintained at:
  \begin{center}
    \url{https://github.com/brettviren/lcatr}    
  \end{center}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Jim's G/N/C Example in the Schema}

  Construct an unfilled schema (only convention-required content added).

  \begin{lstlisting}[emph={GncInputFilesHDU,GncResult,schema,TableHDU,Column}]
#...
class GncInputFilesHDU(schema.TableHDU):
    def __init__(self, filelist = None, sha1list = None, version = 0):
        super(GncInputFilesHDU,self).__init__()
        self.update_ext_name('FileRefs')
        self.update_ext_version(version)
        fc = schema.Column(name='FileName', format='A64', array=filelist)
        hc = schema.Column(name='SHA1Hash', format='A64', array=sha1list)
        self.columns = schema.ColDefs([fc,hc])
        return
    pass
#...
class GncResult(schema.HDUList):
    def __init__(self):
        super(GncResult,self).__init__(hdus = [
                GncPrimaryHDU(),
                GncInputFilesHDU(),
                GncGainsHDU(),
                GncNoiseHDU(),
                GncColdSpotsHDU(),
                ])
        return
    pass
  \end{lstlisting}
  \footnotesize
  \begin{itemize}
  \item   Hard-code GNC's expectation of what HDUs/Columns/Cards/etc are expected.
  \item   Additional GNC-specific validation code can added to these HDU classes.

  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Closer look at the \texttt{pyfits} Bolt-ons}

  \begin{lstlisting}[emph={PrimaryHDU_validate,TableHDU_validate,PrimaryHDU,TableHDU,validate}]
# in lcatr.schema
from pyfits import *

def PrimaryHDU_validate(self):
  self.verify()               # pyfits-level verify
  for name in ['EXTNAME','EXTVER']: 
    self.header[name]         # just a lookup to trigger a check
  return
PrimaryHDU.validate = PrimaryHDU_validate  # bolt-on

def TableHDU_validate(self):
  self.verify()
  for name in ['EXTNAME','EXTVER']: 
    self.header[name]
  if not len(self.columns):
    raise ValueError,'TableHDU "%s": no columns' % self.name
  if not all([c.array for c in self.columns]):
    raise ValueError, 'TableHDU "%s": not all columns have arrays' % self.name
  return
TableHDU.validate = TableHDU_validate  # bolt-on
  \end{lstlisting}
  \begin{itemize}
  \item Define \texttt{validate()} method and bolt on to \texttt{pyfits.PrimaryHDU} class.
  \item Require standard name and version cards.
  \item Expect this to expand as we learn more about the data.
  \end{itemize}
\end{frame}


\begin{frame}[fragile]
  \frametitle{Example of Filling the GNC Schema}
  
  Filling from code:
  \begin{lstlisting}
    gnc = GncResult()
    gnc['Gains'].columns[0].array = [2.5122094, 2.8183959, 2.5010307, ...]
    gnc['Gains'].columns[1].array = [2.6737275, 2.7658350, 2.6715354, ...]
    # ...
    gnc.validate()
  \end{lstlisting}

  Some convenience methods can be added for a nicer filling interface,
  but this shows underlying idea.

  \vspace{3mm}

  Filling and validating a file that is already produced:

  \begin{lstlisting}
    import lcatr.schema
    import pyfits
    gnc = pyfits.open("gnc.root")
    gnc.validate()
  \end{lstlisting}
\end{frame}

\begin{frame}
  \frametitle{Questions on this concept}
  \begin{itemize}
  \item Do we need to separate the Metadata and Result FITS files?
    \begin{itemize}
    \item Can/should the Metadata be put into the Result FITS files?
      \begin{itemize}
      \item Better granularity in referencing.
      \item Fewer schema to specify and parse.
      \item Less files to manage.
      \end{itemize}
    \end{itemize}
  \item How many test stations/analyzers work in Python?  
    \begin{itemize}
    \item We will need intermediate scripts to generate compliant
      Results FITS files from the others.
    \end{itemize}
  \item How likely will it be that the schema needs to evolve over
    time?
    \begin{itemize}
    \item If we can guarantee all tests produce the same type of data,
      life is easier.
    \end{itemize}
  \item What else?
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Input Needed}
  \begin{itemize}
  \item Any criticism on this high-level design so far.
  \item A detailed description of the information expected from each
    test station and analyzer.
    \begin{itemize}
    \item To the level of types and quantity of all data.
    \item Developing each result's schema will require iteration.
    \item I will handle producing a unified document covering the
      information produced from all tests based on what I get.
    \end{itemize}
  \item Work with each test operator/analyzer to help them adopt this
    schema for their results.
    \begin{itemize}
    \item I will help integrate the \texttt{lcatr} package into your
      Python-based tests and/or will help write any needed
      generator/converter scripts for tests that do not use Python.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Some Open Issues That Need Thought}
  \begin{itemize}
  \item Must handle test station calibration data.
    \begin{itemize}
    \item Maybe handle in the same way as result data.
    \end{itemize}
  \item Must handle environmental (netbotz) information.
    \begin{itemize}
    \item Need system of data collection and DB upload.
    \item Assure proper subsequent lookup on timestamp.
      \begin{itemize}
      \item Is NTP running on all test stations and ``netbotz''?
      \item What ongoing assurance that clocks stay synced?
      \end{itemize}
    \end{itemize}
  \end{itemize}
\end{frame}

\end{document}

